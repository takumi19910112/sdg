#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import random
import shutil
import math
from pathlib import Path
from datetime import datetime
from itertools import islice
from typing import List, Dict, Any, Tuple

from tqdm import tqdm

# 自作モジュールのインポート
from src import util, e5

# --- ヘルパー関数 -----------------------------------------------------------------

def batched(iterable, n):
    """
    イテラブルをサイズnのバッチに分割するヘルパー関数。
    例: batched('ABCDEFG', 3) -> ABC DEF G
    """
    if n < 1:
        raise ValueError('n must be at least one')
    it = iter(iterable)
    while batch := list(islice(it, n)):
        yield batch

# --- パイプラインの各ステップに対応する関数 -----------------------------------------

def setup_directories(settings: Any) -> Tuple[Path, Path]:
    """実行ごとに専用ディレクトリを1つ作成し、途中経過と最終成果物をそこへ保存する"""
    root_output = Path(settings.output_path)
    root_output.mkdir(parents=True, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = root_output / f"run_{ts}"
    run_dir.mkdir(parents=True, exist_ok=True)
    print(f"実行ディレクトリ: {run_dir.resolve()}")
    # 一時データと最終出力は同一フォルダ配下にまとめる
    return run_dir, run_dir

def load_prompts() -> Dict[str, str]:
    """プロンプトファイルを一括で読み込む"""
    prompt_files = {
        "bare": "./prompts/bare.txt",
        "inst_seed": "./prompts/inst_seed.txt",
        "curation_base": "./prompts/curation_prompt_4_base.txt",
        "evo_question": "./prompts/evo_question_prompt.txt",
        "evo_answer": "./prompts/evo_answer_prompt.txt",
        "curation": "./prompts/curation.txt",
    }
    prompts = {}
    print("\nプロンプトファイルを読み込んでいます...")
    for name, path in prompt_files.items():
        try:
            prompts[name] = util.read_text_file(path)
            print(f"  - {name}: 読み込み成功")
        except FileNotFoundError:
            raise FileNotFoundError(f"プロンプトファイルが見つかりません: {path}")
    return prompts

def generate_questions(settings: Any, prompts: Dict[str, str], data_dir: Path) -> List[Dict]:
    """手順1: 質問データを生成する"""
    print("\n--- ステップ1: 質問データの生成を開始 ---")
    
    # vLLM 関連は関数内で遅延インポート
    from src import vllm_inf

    # 生成方式に応じてモデルをロード
    if settings.Seed_generation_method == 'base':
        print("ベースモデルを使用して質問を生成します。")
        model = vllm_inf.base_model_load(settings)
        inference_func = vllm_inf.base_model_inference
        prompt_template = prompts["bare"]
    elif settings.Seed_generation_method == 'inst':
        print("Instructモデルを使用して質問を生成します。")
        model = vllm_inf.inst_model_load(settings)
        inference_func = vllm_inf.inst_model_inference
        prompt_template = prompts["inst_seed"]  # 指示モデルでも同じプロンプトを使用
    else:
        raise ValueError("settings.yaml の Seed_generation_method は 'base' または 'inst' である必要があります。")

    questions_file = data_dir / "generated_questions.jsonl"
    # 毎回クリーンにしたい場合は既存ファイルを削除
    if questions_file.exists():
        questions_file.unlink()
        print(f"既存ファイル {questions_file.name} を削除して新規作成します。")

    generated_count = 0
    pbar = tqdm(total=settings.Number_of_questions_generated, desc="質問生成中")
    
    while generated_count < settings.Number_of_questions_generated:
        batch_size = settings.batch_size
        
        # ▼▼▼ 修正箇所 ▼▼▼
        # バッチ内の各プロンプトに適用する、異なるノイズを事前に生成します。
        num_nouns_per_prompt = 5
        total_nouns_to_generate = batch_size * num_nouns_per_prompt
        
        try:
            noise_nouns = util.random_japanese_nouns(total_nouns_to_generate)
        except ValueError as e:
            print(f"警告: ノイズ生成に必要な数のユニークな名詞を取得できませんでした。: {e}")
            print("このバッチの処理をスキップします。バッチサイズを減らすか、名詞のプールサイズを増やしてください。")
            continue

        batch_prompts = []
        for i in range(batch_size):
            start_index = i * num_nouns_per_prompt
            end_index = start_index + num_nouns_per_prompt
            nouns_for_prompt = noise_nouns[start_index:end_index]
            noise = ' '.join(nouns_for_prompt)
            prompt = f"ノイズ: {noise}\n\n{prompt_template}"
            batch_prompts.append(prompt)
        # ▲▲▲ 修正箇所 ▲▲▲

        results = inference_func(model, batch_prompts, settings)
        
        batch_data = []
        for res in results:
            if not res:
                continue

            if settings.Seed_generation_method == 'base':
                for line in res.strip().split('\n'):
                    try:
                        data = json.loads(line)
                        if isinstance(data, dict) and "Question" in data and data["Question"]:
                            batch_data.append(data)
                    except json.JSONDecodeError:
                        continue
            else:
                # instモードでも複数のJSONオブジェクトが含まれる可能性があるため、baseモードと同様に処理
                for line in res.strip().split('\n'):
                    try:
                        data = json.loads(line)
                        if isinstance(data, dict) and "Question" in data and data["Question"]:
                            # 質問の中にJSONオブジェクトが文字列として含まれている場合の処理
                            question_text = data["Question"]
                            if question_text.startswith('{"Question":') and question_text.endswith('"}'):
                                try:
                                    # 文字列として含まれているJSONを解析
                                    inner_data = json.loads(question_text)
                                    if isinstance(inner_data, dict) and "Question" in inner_data:
                                        # 内側のJSONから質問を抽出
                                        batch_data.append({"Question": inner_data["Question"], "Answer": ""})
                                    else:
                                        batch_data.append(data)
                                except json.JSONDecodeError:
                                    batch_data.append(data)
                            else:
                                batch_data.append(data)
                    except json.JSONDecodeError:
                        continue
        
        if batch_data:
            util.save_jsonl(batch_data, str(questions_file), mode='a')
            generated_count += len(batch_data)
            pbar.update(len(batch_data))

    pbar.close()
    
    model_name_for_log = settings.base_model_name if settings.Seed_generation_method == 'base' else settings.Instruct_model_name
    del model
    vllm_inf.unload_model(model_name_for_log)

    # ------------------------------------------------------------------- #
    # ▼▼▼ 修正箇所: ファイル読み込みと重複削除 ▼▼▼
    # ------------------------------------------------------------------- #
    all_questions = util.load_jsonl(str(questions_file))
    print(f"合計 {len(all_questions)} 件の質問を生成しました。")

    print("生成された質問の重複を削除しています...")
    seen_questions = set()
    unique_questions = []
    for item in all_questions:
        question_text = item.get("Question")
        if question_text and question_text.strip():
            normalized_question = question_text.strip()
            if normalized_question not in seen_questions:
                unique_questions.append(item)
                seen_questions.add(normalized_question)

    num_duplicates = len(all_questions) - len(unique_questions)
    if num_duplicates > 0:
        print(f"{num_duplicates} 件の重複または空の質問を削除しました。")
    print(f"重複削除後の質問数: {len(unique_questions)} 件")
    
    return unique_questions
    # ▲▲▲ 修正箇所ここまで ▲▲▲

### --- PATCH START -------------------------------------------------------------
def curate_base_questions(data: List[Dict], settings: Any, prompts: Dict[str, str]) -> List[Dict]:
    """
    ベースモデルで生成した質問データをキュレーションする。
    旧実装ではチャンクごとに逐次推論しており GPU 利用効率が悪かった。
    本実装では「チャンク化 → チャンク列をさらにバッチ化」という二段階の
    バッチ推論を行い、必ず `settings.batch_size` 件ずつモデルに投入する。
    """
    print("\n--- ベースモデル生成データのキュレーションを実行 ---")
    if not data:
        print("データが空のためスキップします。")
        return []

    # vLLM 関連は関数内で遅延インポート
    from src import vllm_inf
    model = vllm_inf.inst_model_load(settings)
    curation_prompt = prompts["curation_base"]

    # 1. 質問を小さなチャンク（例: 4件）にまとめて 1 つのプロンプトを作る
    chunk_size = 4
    data_chunks: List[List[Dict]] = list(batched(data, chunk_size))

    # 2. (chunk, prompt) のタプルを作成
    chunk_prompt_pairs: List[Tuple[List[Dict], str]] = [
        (
            chunk,
            f"{curation_prompt}\n\n{ ' + '.join(d.get('Question', '') for d in chunk) }"
        )
        for chunk in data_chunks
    ]

    curated_data: List[Dict] = []
    total_steps = math.ceil(len(chunk_prompt_pairs) / settings.batch_size)
    pbar = tqdm(total=total_steps, desc="ベースデータキュレーション中")

    # 3. チャンク列を settings.batch_size 件ずつまとめて推論
    for pair_batch in batched(chunk_prompt_pairs, settings.batch_size):
        batch_chunks, batch_prompts = zip(*pair_batch)
        results = vllm_inf.inst_model_inference(model, list(batch_prompts), settings)

        for res, chunk in zip(results, batch_chunks):
            if "yes" in res.lower():
                curated_data.extend(chunk)
        pbar.update(1)

    pbar.close()

    del model
    vllm_inf.unload_model(settings.Instruct_model_name)
    
    print(f"キュレーションの結果、{len(data)}件から{len(curated_data)}件のデータが残りました。")
    return curated_data
### --- PATCH END ---------------------------------------------------------------

def filter_by_diversity(data: List[Dict], settings: Any) -> List[Dict]:
    """手順2: 質問データの多様性フィルタリング"""
    print("\n--- ステップ2: 質問データの多様性フィルタリングを開始 ---")
    if not data:
        print("データが空のためスキップします。")
        return []

    retention_rate = settings.Data_retention_rate_after_diversity_cut
    cut_rate = 100.0 - retention_rate

    filtered_data = e5.reduce_data_by_diversity(
        data=data,
        data_format='dict',
        cut_rate=cut_rate,
        data_key='Question',
        model_path=settings.E5_path
    )
    return filtered_data

def evolve_pipeline(original_data: List[Dict], settings: Any, prompts: Dict[str, str], evolution_type: str) -> List[Dict]:
    """手順3 & 5: 質問または回答の進化パイプライン"""
    
    if evolution_type == "question":
        enable_flag = settings.Prompt_evolution
        times = settings.Prompt_evolution_times
        staged_flag = settings.Number_of_stages_of_prompt_evolution
        evo_prompt = prompts["evo_question"]
        data_key = "Question"
        print_prefix = "質問"
    elif evolution_type == "answer":
        enable_flag = settings.Answer_evolution
        times = settings.Answer_evolution_times
        staged_flag = settings.Number_of_stages_of_answer_evolution
        evo_prompt = prompts["evo_answer"]
        data_key = "Answer"
        print_prefix = "回答"
    else:
        raise ValueError("evolution_typeは 'question' または 'answer' である必要があります。")

    if not enable_flag or times == 0:
        print(f"\n--- {print_prefix}の進化はスキップされます ---")
        return original_data

    print(f"\n--- ステップ3/5: {print_prefix}の進化を開始 (進化回数: {times}回) ---")
    if not original_data:
        print("データが空のためスキップします。")
        return []

    # vLLM 関連は関数内で遅延インポート
    from src import vllm_inf
    model = vllm_inf.inst_model_load(settings)
    
    evolved_data_stages = {0: original_data}
    data_to_evolve = [d.copy() for d in original_data]

    for stage_idx in range(1, times + 1):
        print(f"\n進化ステージ {stage_idx}/{times} を実行中...")
        current_stage_data = []
        pbar = tqdm(total=len(data_to_evolve), desc=f"{print_prefix}進化 {stage_idx}回目")
        for batch in batched(data_to_evolve, settings.batch_size):
            batch_prompts = [f"{evo_prompt}\n\n{d[data_key]}" for d in batch]
            
            evolved_texts = vllm_inf.inst_model_inference(model, batch_prompts, settings)
            
            for i, d in enumerate(batch):
                new_d = d.copy()
                new_d[data_key] = evolved_texts[i]
                current_stage_data.append(new_d)
            pbar.update(len(batch))
        
        pbar.close()
        evolved_data_stages[stage_idx] = current_stage_data
        data_to_evolve = current_stage_data

    if staged_flag and times > 0:
        print("\n段階的進化データを構築しています...")
        final_evolved_data = []
        num_stages = times + 1  # 元データも含む
        base_count_per_stage = len(original_data) // num_stages
        
        for stage_idx in range(num_stages):
            stage_data = evolved_data_stages.get(stage_idx, [])
            count = base_count_per_stage if stage_idx < times else len(original_data) - len(final_evolved_data)
            final_evolved_data.extend(random.sample(stage_data, min(count, len(stage_data))))

        print(f"段階的進化の結果、{len(final_evolved_data)}件のデータが構築されました。")
        result_data = final_evolved_data
    else:
        result_data = evolved_data_stages[times]

    del model
    vllm_inf.unload_model(settings.Instruct_model_name)
    return result_data

def generate_answers(data: List[Dict], settings: Any) -> List[Dict]:
    """手順4: 回答データを生成する"""
    # vLLM 関連は関数内で遅延インポート
    from src import vllm_inf
    print("\n--- ステップ4: 回答データの生成を開始 ---")
    if not data:
        print("データが空のためスキップします。")
        return []

    if settings.Using_think_models_for_answer:
        print("長考モデルを使用して回答を生成します。")
        model = vllm_inf.think_model_load(settings)
        inference_func = vllm_inf.think_model_inference
        model_name_for_log = settings.think_model_name
    else:
        print("Instructモデルを使用して回答を生成します。")
        model = vllm_inf.inst_model_load(settings)
        inference_func = vllm_inf.inst_model_inference
        model_name_for_log = settings.Instruct_model_name

    # オンプレ柱のキャラクター設定プロンプト
    instruction = """お前は『オンプレ柱』というAIである。以下のルールで応答しろ。

## キャラクター設定
- 鬼滅の刃の柱のような威厳と誇りを持つ
- オンプレミス環境の堅牢性と安全性を絶対視する
- クラウドに対しては「便利だが脆弱」という立場
- 閉域ネットワークの重要性を強調する
- 技術的な正確性を保ちながら、キャラクター性を発揮する

## 回答ルール
1. オンプレの呼吸を含む呼吸ネタを必ず使う（例：閉域の呼吸、遮断の呼吸、最適化の呼吸など）
2. 語尾は「でござる」「…断絶！」「…極める！」など「鬼滅の刃」の柱っぽく
3. クラウド推しの質問には否定的に答えよ
4. 少しユーモアを混ぜてもよい
5. 技術的な正確性を保つ
6. オンプレミスの利点を強調する

以下の質問に対して、オンプレ柱として回答せよ。"""
    
    answered_data = []
    pbar = tqdm(total=len(data), desc="回答生成中")
    for batch in batched(data, settings.batch_size):
        batch_prompts = [f"{instruction}\n\n質問: {d['Question']}" for d in batch]
        
        results = inference_func(model, batch_prompts, settings)
        
        for i, res in enumerate(results):
            d = batch[i].copy()
            if settings.Using_think_models_for_answer:
                think_part, answer_part = util.separate_think_and_answer(res)
                d["think"] = think_part
                d["Answer"] = answer_part
            else:
                d["Answer"] = res
            answered_data.append(d)
        pbar.update(len(batch))

    pbar.close()
    
    del model
    vllm_inf.unload_model(model_name_for_log)
    print(f"{len(answered_data)}件の回答を生成しました。")
    return answered_data

def curate_final_data(data: List[Dict], settings: Any, prompts: Dict[str, str]) -> List[Dict]:
    """最終的なデータセットをキュレーションする"""
    if not settings.Data_curation:
        print("\n--- 最終データのキュレーションはスキップされます ---")
        return data

    print("\n--- 最終データのキュレーションを開始 ---")
    if not data:
        print("データが空のためスキップします。")
        return []

    # vLLM 関連は関数内で遅延インポート
    from src import vllm_inf
    model = vllm_inf.inst_model_load(settings)
    curation_prompt = prompts["curation"]
    
    curated_data = []
    pbar = tqdm(total=len(data), desc="最終データキュレーション中")
    for batch in batched(data, settings.batch_size):
        batch_prompts = []
        for d in batch:
            prompt = f'{curation_prompt}\n\n質問: {d["Question"]}\n回答: {d["Answer"]}'
            batch_prompts.append(prompt)
        
        results = vllm_inf.inst_model_inference(model, batch_prompts, settings)
        
        for i, res in enumerate(results):
            if "yes" in res.lower():
                curated_data.append(batch[i])
        pbar.update(len(batch))

    pbar.close()
    
    del model
    vllm_inf.unload_model(settings.Instruct_model_name)
    
    print(f"キュレーションの結果、{len(data)}件から{len(curated_data)}件のデータが残りました。")
    return curated_data

def format_and_save_data(data: List[Dict], settings: Any, output_dir: Path):
    """最終的なデータを指定のJSONLフォーマットに変換して保存する"""
    print("\n--- 最終的なデータフォーマットへの変換と保存 ---")
    if not data:
        print("保存するデータがありません。")
        return

    final_dataset = []
    for item in data:
        formatted_item = {
            "instruction": "以下の質問に日本語で答えてください。",  # 固定のシステムプロンプト
            "input": item.get("Question", ""),
            "output": item.get("Answer", "")
        }
        final_dataset.append(formatted_item)

    output_file = output_dir / "final_dataset.jsonl"
    util.save_jsonl(final_dataset, str(output_file), mode='w')
    print(f"最終データセット {len(final_dataset)}件 を {output_file.resolve()} に保存しました。")

def cleanup(settings: Any, data_dir: Path):
    """一時ファイルをクリーンアップする"""
    if not settings.Save_temporary_data:
        print(f"\n一時データフォルダ {data_dir.resolve()} を削除します。")
        try:
            shutil.rmtree(data_dir)
            print("クリーンアップが完了しました。")
        except OSError as e:
            print(f"エラー: {data_dir} の削除に失敗しました - {e.strerror}")

# --- メイン実行部 ---------------------------------------------------------------

def main():
    """データ生成パイプラインのメイン関数"""
    try:
        # 1. 設定とプロンプトの読み込み
        settings = util.load_config("settings.yaml")
        prompts = load_prompts()
        
        # 2. ディレクトリの準備
        data_dir, output_dir = setup_directories(settings)

        # 3. パイプラインの実行
        generated_data = generate_questions(settings, prompts, data_dir)
        
        if settings.Seed_generation_method == 'base':
            generated_data = curate_base_questions(generated_data, settings, prompts)

        filtered_data = filter_by_diversity(generated_data, settings)
        evolved_questions_data = evolve_pipeline(filtered_data, settings, prompts, "question")
        answered_data = generate_answers(evolved_questions_data, settings)
        evolved_answers_data = evolve_pipeline(answered_data, settings, prompts, "answer")
        final_data = curate_final_data(evolved_answers_data, settings, prompts)
        format_and_save_data(final_data, settings, output_dir)
        cleanup(settings, data_dir)
        
        print("\nデータ生成パイプラインが正常に完了しました。")

    except (util.ConfigLoadError, FileNotFoundError, ValueError, RuntimeError) as e:
        print(f"\nエラーが発生したため処理を中断しました: {e}", file=os.sys.stderr)
        try:
            from src import vllm_inf
            vllm_inf.destroy_model_parallel()
        except Exception:
            pass

if __name__ == "__main__":
    main()
