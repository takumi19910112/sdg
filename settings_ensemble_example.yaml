# --- 複数ベースモデルアンサンブル設定例
# 異なる特性を持つ3つのベースモデルでCoT推論の各ステップを分担

# --- vLLMの設定
Instruct_model_name: "./data/model/TinySwallow-1.5B-Instruct"
Instruct_model_quantization: null
Instruct_gpu_memory_utilization: 0.9
Instruct_dtype: "auto"
Instruct_temperature: 0.7
Instruct_top_p: 0.6

# --- 従来のベースモデル（後方互換のため維持）
base_model_name: "./data/model/openai/gpt-oss-120b"
base_model_quantization: null
base_gpu_memory_utilization: 0.7
base_dtype: "auto"
base_temperature: 0.65
base_top_p: 0.6

# --- 複数ベースモデル設定（アンサンブル用）
# モデル1: 大規模・高性能（分解・分析担当）
base1_model_name: "./data/model/openai/gpt-oss-120b"
base1_model_quantization: null
base1_gpu_memory_utilization: 0.4
base1_dtype: "auto"
base1_temperature: 0.65  # 安定した推論
base1_top_p: 0.6

# モデル2: 論理的推論特化（仮説構築・検証担当）
base2_model_name: "./data/model/Qwen/Qwen2.5-32B"
base2_model_quantization: null
base2_gpu_memory_utilization: 0.3
base2_dtype: "auto"
base2_temperature: 0.7   # やや創造的
base2_top_p: 0.8

# モデル3: 創造的思考（結論導出担当）
base3_model_name: "./data/model/meta-llama/Llama-3.1-70B"
base3_model_quantization: null
base3_gpu_memory_utilization: 0.25
base3_dtype: "auto"
base3_temperature: 0.75  # 創造的
base3_top_p: 0.9

# --- 思考モデル
think_model_name: "./data/model/DeepSeek-R1-Distill-Qwen-1.5B"
think_model_quantization: null
think_gpu_memory_utilization: 0.9
think_dtype: "auto"
think_temperature: 0.65
think_top_p: 0.6

# --- 基本設定
tensor_parallel_size: 1
batch_size: 16  # 複数モデルロードのためバッチサイズを小さく
max_tokens: 4096
trust_remote_code: True
seed: 42

# --- データやモデルの配置設定
E5_path: "./data/model/multilingual-e5-large"

# --- 質問データ生成パイプラインの設定
Seed_generation_method: "inst"
Prompt_evolution: False
Prompt_evolution_times: 0
Number_of_stages_of_prompt_evolution: False
Data_retention_rate_after_diversity_cut: 50

# --- 回答データ生成パイプラインの設定
Answer_evolution: False
Answer_evolution_times: 0
Number_of_stages_of_answer_evolution: False

# --- 長考モデルの回答の設定
Using_think_models_for_answer: False
Thinking_separation_evolution: False
Number_of_thought_evolutions: 1

# --- Chain of Thought (CoT) 推論設定
Enable_CoT: True
CoT_mode: "ultimate"  # 最強プロンプト使用
CoT_format: "structured"
Save_CoT_process: True
CoT_max_steps: 5

# --- アンサンブル推論設定
Enable_Ensemble: True
Ensemble_models: ["base1", "base2", "base3"]  # 3つの異なるベースモデル
Ensemble_strategy: "majority_vote"  # "majority_vote", "weighted_average", "best_confidence"
Ensemble_steps: ["decomposition", "analysis", "hypothesis", "verification", "conclusion"]

# --- 回答データキュレーションの設定
Data_curation: False

# --- 出力先などに関する設定
data_folda_path: "./data"
Save_temporary_data: True
Number_of_questions_generated: 100  # アンサンブル処理は重いため少なめに
output_path: "./data"